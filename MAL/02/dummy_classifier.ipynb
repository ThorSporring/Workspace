{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Implementing a dummy binary-classifier with fit-predict interface\n",
    "\n",
    "We begin with the MNIST data-set and will reuse the data loader from Scikit-learn. Next we create a dummy classifier, and compare the results of the SGD and dummy classifiers using the MNIST data...\n",
    "\n",
    "#### Qa  Load and display the MNIST data\n",
    "\n",
    "There is a `sklearn.datasets.fetch_openml` dataloader interface in Scikit-learn. You can load MNIST data like \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784',??) # needs to return X, y, replace '??' with suitable parameters! \n",
    "# Convert to [0;1] via scaling (not always needed)\n",
    "#X = X / 255.\n",
    "```\n",
    "\n",
    "but you need to set parameters like `return_X_y` and `cache` if the default values are not suitable! \n",
    "\n",
    "Check out the documentation for the `fetch_openml` MNIST loader, try it out by loading a (X,y) MNIST data set, and plot a single digit via the `MNIST_PlotDigit` function here (input data is a 28x28 NMIST subimage)\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "```\n",
    "\n",
    "Finally, put the MNIST loader into a single function called `MNIST_GetDataSet()` so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: add your code here..\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_openml\n\u001b[1;32m----> 3\u001b[0m X, y \u001b[38;5;241m=\u001b[39m fetch_openml(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist_784\u001b[39m\u001b[38;5;124m'\u001b[39m, return_X_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, data_home \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMNIST_PlotDigit\u001b[39m(data):\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1118\u001b[0m, in \u001b[0;36mfetch_openml\u001b[1;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;66;03m# obtain the data\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m url \u001b[38;5;241m=\u001b[39m _DATA_FILE\u001b[38;5;241m.\u001b[39mformat(data_description[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1118\u001b[0m bunch \u001b[38;5;241m=\u001b[39m _download_data_to_bunch(\n\u001b[0;32m   1119\u001b[0m     url,\n\u001b[0;32m   1120\u001b[0m     return_sparse,\n\u001b[0;32m   1121\u001b[0m     data_home,\n\u001b[0;32m   1122\u001b[0m     as_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(as_frame),\n\u001b[0;32m   1123\u001b[0m     openml_columns_info\u001b[38;5;241m=\u001b[39mfeatures_list,\n\u001b[0;32m   1124\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   1125\u001b[0m     target_columns\u001b[38;5;241m=\u001b[39mtarget_columns,\n\u001b[0;32m   1126\u001b[0m     data_columns\u001b[38;5;241m=\u001b[39mdata_columns,\n\u001b[0;32m   1127\u001b[0m     md5_checksum\u001b[38;5;241m=\u001b[39mdata_description[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmd5_checksum\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1128\u001b[0m     n_retries\u001b[38;5;241m=\u001b[39mn_retries,\n\u001b[0;32m   1129\u001b[0m     delay\u001b[38;5;241m=\u001b[39mdelay,\n\u001b[0;32m   1130\u001b[0m     parser\u001b[38;5;241m=\u001b[39mparser_,\n\u001b[0;32m   1131\u001b[0m     read_csv_kwargs\u001b[38;5;241m=\u001b[39mread_csv_kwargs,\n\u001b[0;32m   1132\u001b[0m )\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_X_y:\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bunch\u001b[38;5;241m.\u001b[39mdata, bunch\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:669\u001b[0m, in \u001b[0;36m_download_data_to_bunch\u001b[1;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserError\n\u001b[0;32m    667\u001b[0m     no_retry_exception \u001b[38;5;241m=\u001b[39m ParserError\n\u001b[1;32m--> 669\u001b[0m X, y, frame, categories \u001b[38;5;241m=\u001b[39m _retry_with_clean_cache(\n\u001b[0;32m    670\u001b[0m     url, data_home, no_retry_exception\n\u001b[0;32m    671\u001b[0m )(_load_arff_response)(\n\u001b[0;32m    672\u001b[0m     url,\n\u001b[0;32m    673\u001b[0m     data_home,\n\u001b[0;32m    674\u001b[0m     parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    675\u001b[0m     output_type\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[0;32m    676\u001b[0m     openml_columns_info\u001b[38;5;241m=\u001b[39mfeatures_dict,\n\u001b[0;32m    677\u001b[0m     feature_names_to_select\u001b[38;5;241m=\u001b[39mdata_columns,\n\u001b[0;32m    678\u001b[0m     target_names_to_select\u001b[38;5;241m=\u001b[39mtarget_columns,\n\u001b[0;32m    679\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m    680\u001b[0m     md5_checksum\u001b[38;5;241m=\u001b[39mmd5_checksum,\n\u001b[0;32m    681\u001b[0m     n_retries\u001b[38;5;241m=\u001b[39mn_retries,\n\u001b[0;32m    682\u001b[0m     delay\u001b[38;5;241m=\u001b[39mdelay,\n\u001b[0;32m    683\u001b[0m     read_csv_kwargs\u001b[38;5;241m=\u001b[39mread_csv_kwargs,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Bunch(\n\u001b[0;32m    687\u001b[0m     data\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    688\u001b[0m     target\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m     target_names\u001b[38;5;241m=\u001b[39mtarget_columns,\n\u001b[0;32m    693\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:59\u001b[0m, in \u001b[0;36m_retry_with_clean_cache.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:534\u001b[0m, in \u001b[0;36m_load_arff_response\u001b[1;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay, read_csv_kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m arff_params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    525\u001b[0m     parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    526\u001b[0m     output_type\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    531\u001b[0m     read_csv_kwargs\u001b[38;5;241m=\u001b[39mread_csv_kwargs \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    532\u001b[0m )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     X, y, frame, categories \u001b[38;5;241m=\u001b[39m _open_url_and_load_gzip_file(\n\u001b[0;32m    535\u001b[0m         url, data_home, n_retries, delay, arff_params\n\u001b[0;32m    536\u001b[0m     )\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:522\u001b[0m, in \u001b[0;36m_load_arff_response.<locals>._open_url_and_load_gzip_file\u001b[1;34m(url, data_home, n_retries, delay, arff_params)\u001b[0m\n\u001b[0;32m    520\u001b[0m gzip_file \u001b[38;5;241m=\u001b[39m _open_openml_url(url, data_home, n_retries\u001b[38;5;241m=\u001b[39mn_retries, delay\u001b[38;5;241m=\u001b[39mdelay)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m closing(gzip_file):\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_arff_from_gzip_file(gzip_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marff_params)\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_arff_parser.py:520\u001b[0m, in \u001b[0;36mload_arff_from_gzip_file\u001b[1;34m(gzip_file, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, read_csv_kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a compressed ARFF file using a given parser.\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    `output_array_type == \"pandas\"`.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliac-arff\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _liac_arff_parser(\n\u001b[0;32m    521\u001b[0m         gzip_file,\n\u001b[0;32m    522\u001b[0m         output_type,\n\u001b[0;32m    523\u001b[0m         openml_columns_info,\n\u001b[0;32m    524\u001b[0m         feature_names_to_select,\n\u001b[0;32m    525\u001b[0m         target_names_to_select,\n\u001b[0;32m    526\u001b[0m         shape,\n\u001b[0;32m    527\u001b[0m     )\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parser \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_arff_parser(\n\u001b[0;32m    530\u001b[0m         gzip_file,\n\u001b[0;32m    531\u001b[0m         output_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    535\u001b[0m         read_csv_kwargs,\n\u001b[0;32m    536\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_arff_parser.py:197\u001b[0m, in \u001b[0;36m_liac_arff_parser\u001b[1;34m(gzip_file, output_arrays_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape)\u001b[0m\n\u001b[0;32m    195\u001b[0m columns_to_keep \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns_names \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns_to_select]\n\u001b[0;32m    196\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [first_df[columns_to_keep]]\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m _chunk_generator(arff_container[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], chunksize):\n\u001b[0;32m    198\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    199\u001b[0m         pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumns_names, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[columns_to_keep]\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# dfs[0] contains only one row, which may not have enough data to infer to\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# column's dtype. Here we use `dfs[1]` to configure the dtype in dfs[0]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:729\u001b[0m, in \u001b[0;36m_chunk_generator\u001b[1;34m(gen, chunksize)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03mchunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(islice(gen, chunksize))\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[0;32m    731\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\externals\\_arff.py:461\u001b[0m, in \u001b[0;36mDenseGeneratorData.decode_rows\u001b[1;34m(self, stream, conversors)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_rows\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream, conversors):\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m    462\u001b[0m         values \u001b[38;5;241m=\u001b[39m _parse_values(row)\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\externals\\_arff.py:864\u001b[0m, in \u001b[0;36mArffDecoder._decode.<locals>.stream\u001b[1;34m()\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m():\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_line \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    866\u001b[0m         row \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_arff_parser.py:161\u001b[0m, in \u001b[0;36m_liac_arff_parser.<locals>._io_to_generator\u001b[1;34m(gzip_file)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_io_to_generator\u001b[39m(gzip_file):\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m gzip_file:\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\gzip.py:399\u001b[0m, in \u001b[0;36mGzipFile.readline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadline\u001b[39m(\u001b[38;5;28mself\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_not_closed()\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mreadline(size)\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(byte_view))\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\spott\\anaconda3\\Lib\\gzip.py:507\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[0;32m    505\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[1;32m--> 507\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(buf, size)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: add your code here..\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', return_X_y = True, data_home = \"./data\", cache=True)\n",
    "X = X/255\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.to_numpy().reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "MNIST_PlotDigit(X[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb  Add a Stochastic Gradient Decent [SGD] Classifier\n",
    "\n",
    "Create a train-test data-set for MNIST and then add the `SGDClassifier` as done in [HOML], p.103.\n",
    "\n",
    "Split your data and run the fit-predict for the classifier using the MNIST data.(We will be looking at cross-validation instead of the simple fit-predict in a later exercise.)\n",
    "\n",
    "Notice that you have to reshape the MNIST X-data to be able to use the classifier. It may be a 3D array, consisting of 70000 (28 x 28) images, or just a 2D array consisting of 70000 elements of size 784.\n",
    "\n",
    "A simple `reshape()` could fix this on-the-fly:\n",
    "```python\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "print(f\"X.shape={X.shape}\") # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(f\"X.shape={X.shape}\") # X.shape= (70000, 784)\n",
    "```\n",
    "\n",
    "Remember to use the category-5 y inputs\n",
    "\n",
    "```python\n",
    "y_train_5 = (y_train == '5')    \n",
    "y_test_5  = (y_test == '5')\n",
    "```\n",
    "instead of the `y`'s you are getting out of the dataloader. In effect, we have now created a binary-classifier, that enable us to classify a particular data sample, $\\mathbf{x}(i)$ (that is a 28x28 image), as being a-class-5 or not-a-class-5. \n",
    "\n",
    "Test your model on using the test data, and try to plot numbers that have been categorized correctly. Then also find and plots some misclassified numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: solve Qb, and remove me..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc Implement a dummy binary classifier\n",
    "\n",
    "Now we will try to create a Scikit-learn compatible estimator implemented via a python class. Follow the code found in [HOML], p.107 (for [HOML] 1st and 2nd editions: name you estimator `DummyClassifier` instead of `Never5Classifyer`).\n",
    "\n",
    "Here our Python class knowledge comes into play. The estimator class hierarchy looks like\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L02/Figs/class_base_estimator.png\" alt=\"WARNING: could not get image from server.\" style=\"width:500px\">\n",
    "\n",
    "All Scikit-learn classifiers inherit from `BaseEstimator` (and possibly also `ClassifierMixin`), and they must have a `fit-predict` function pair (strangely not in the base class!) and you can actually find the `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin` python source code somewhere in you anaconda install dir, if you should have the nerves to go to such interesting details.\n",
    "\n",
    "But surprisingly you may just want to implement a class that contains the `fit-predict` functions, ___without inheriting___ from the `BaseEstimator`, things still work due to the pythonic 'duck-typing': you just need to have the class implement the needed interfaces, obviously `fit()` and `predict()` but also the more obscure `get_params()` etc....then the class 'looks like' a `BaseEstimator`...and if it looks like an estimator, it _is_ an estimator (aka. duck typing).\n",
    "\n",
    "Templates in C++ also allow the language to use compile-time duck typing!\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Duck_typing\n",
    "\n",
    "Call the fit-predict on a newly instantiated `DummyClassifier` object, and find a way to extract the accuracy `score` from the test data. You may implement an accuracy function yourself or just use the `sklearn.metrics.accuracy_score` function. \n",
    "\n",
    "Finally, compare the accuracy score from your `DummyClassifier` with the scores found in [HOML] \"Measuring Accuracy Using Cross-Validation\", p.107. Are they comparable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: solve Qc, and remove me..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Conclusion\n",
    "\n",
    "Now, conclude on all the exercise above. \n",
    "\n",
    "Write a short textual conclusion (max. 10- to 20-lines) that extract the _essence_ of the exercises: why did you think it was important to look at these particular ML concepts, and what was our overall learning outcome of the exercises (in broad terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd concluding remarks in text.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2018-12-19| CEF, initial.                  \n",
    "2018-02-06| CEF, updated and spell checked. \n",
    "2018-02-08| CEF, minor text update.\n",
    "2018-03-05| CEF, updated with SHN comments.\n",
    "2019-09-02| CEF, updated for ITMAL v2.\n",
    "2019-09-04| CEF, updated and added conclusion Q.\n",
    "2020-01-25| CEF, F20 ITMAL update.\n",
    "2020-02-04| CEF, updated page numbers to HOMLv2.\n",
    "2020-09-03| CEF, E20 ITMAL update, udpated figs paths.\n",
    "2020-09-06| CEF, added alt text.\n",
    "2020-09-18| CEF, added binary-classifier text to Qb to emphasise 5/non-5 classification.\n",
    "2021-01-12| CEF, F21 ITMAL update, moved revision tabel.\n",
    "2021-08-02| CEF, update to E21 ITMAL.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2023-02-07| CEF, update HOML page numbers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.517px",
    "left": "1230px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
